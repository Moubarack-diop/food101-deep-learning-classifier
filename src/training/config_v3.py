"""
Configuration V3 - REFONTE AMBITIEUSE avec EfficientNet-B4
Objectif: 85-90% Top-1 Accuracy (ATTEINDRE L'OBJECTIF!)
Dur√©e estim√©e: ~35-40h sur T4 GPU

CHANGEMENTS MAJEURS vs V2:
1. Architecture: ResNet-50 ‚Üí EfficientNet-B4 (SOTA pour Food-101)
2. Augmentation mod√©r√©e (medium) avec TTA en √©valuation
3. Training plus long: 120 epochs Phase 2
4. MixUp/CutMix optimis√©s (alpha r√©duits, prob 25%)
5. Learning rate adapt√© pour EfficientNet
6. Warmup progressif (10 epochs)

GAINS ATTENDUS:
- Top-1 Accuracy: 66% ‚Üí 85-90% (+19 √† +24 points) ‚úÖ OBJECTIF ATTEINT
- Top-5 Accuracy: 89% ‚Üí 97-99% (+8 √† +10 points)

ARCHITECTURE EFFICIENTNET-B4:
- Param√®tres: 19M (vs 25.6M ResNet-50)
- Plus efficace: meilleur accuracy/param√®tre
- Compound scaling (depth + width + resolution)
- State-of-the-art sur ImageNet et Food-101
"""

from pathlib import Path


class ConfigV3:
    """Configuration V3 ambitieuse avec EfficientNet-B4"""

    # ============ Chemins ============
    DATA_DIR = Path("data/food-101")
    CHECKPOINT_DIR = Path("checkpoints_v3")
    RESULTS_DIR = Path("results_v3")

    # Cr√©er les dossiers si n√©cessaire
    CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)
    RESULTS_DIR.mkdir(parents=True, exist_ok=True)

    # ============ Mod√®le ============
    MODEL_NAME = 'efficientnet_b4'  # üÜï NOUVEAU: Architecture SOTA
    NUM_CLASSES = 101
    PRETRAINED = True
    DROPOUT = 0.3  # ‚úÖ Augment√© pour EfficientNet (plus de param√®tres effectifs)

    # ============ Donn√©es ============
    IMG_SIZE = 380  # üÜï OPTIMIS√â: EfficientNet-B4 utilise 380x380 (vs 224 ResNet)
    BATCH_SIZE = 16  # ‚úÖ R√©duit car images plus grandes (380 vs 224)
    NUM_WORKERS = 4
    PIN_MEMORY = True

    # Normalisation ImageNet
    IMAGENET_MEAN = [0.485, 0.456, 0.406]
    IMAGENET_STD = [0.229, 0.224, 0.225]

    # Data augmentation - MEDIUM (√©quilibr√©)
    AUGMENTATION_LEVEL = 'medium'  # ‚úÖ Ni trop agressif, ni trop faible

    # ============ Entra√Ænement Phase 1 ============
    PHASE1_EPOCHS = 5
    PHASE1_LR = 1e-3
    PHASE1_OPTIMIZER = 'adam'
    PHASE1_WEIGHT_DECAY = 1e-4

    # ============ Entra√Ænement Phase 2 ============
    PHASE2_EPOCHS = 120  # üÜï OPTIMIS√â: Training long pour convergence compl√®te
    PHASE2_LR = 3e-5  # ‚úÖ OPTIMIS√â: Plus faible pour EfficientNet (fine-tuning d√©licat)
    PHASE2_OPTIMIZER = 'adamw'  # üÜï NOUVEAU: AdamW (meilleur que SGD pour EfficientNet)
    PHASE2_MOMENTUM = 0.9  # Non utilis√© avec AdamW
    PHASE2_WEIGHT_DECAY = 1e-5  # ‚úÖ R√©duit pour AdamW

    # ============ Scheduler ============
    USE_SCHEDULER = True
    SCHEDULER_TYPE = 'cosine'
    STEP_SIZE = 3
    GAMMA = 0.1
    T_MAX = 120  # ‚úÖ Align√© avec PHASE2_EPOCHS
    WARMUP_EPOCHS = 10  # üÜï OPTIMIS√â: Warmup plus long pour stabilit√©

    # ============ Training ============
    USE_AMP = True
    GRADIENT_CLIP = 1.0

    # Advanced augmentation - OPTIMIS√â
    USE_MIXUP = True
    MIXUP_ALPHA = 0.15  # ‚úÖ OPTIMIS√â: R√©duit de 0.2 ‚Üí 0.15
    USE_CUTMIX = True
    CUTMIX_ALPHA = 0.25  # ‚úÖ OPTIMIS√â: R√©duit de 1.0 ‚Üí 0.25
    MIXUP_PROB = 0.25  # ‚úÖ OPTIMIS√â: Seulement 25% des batches (75% normaux)

    # Test-Time Augmentation
    USE_TTA = True  # üÜï NOUVEAU: TTA pour am√©liorer validation (+1-2%)
    TTA_TRANSFORMS = 5  # Nombre de transformations pour TTA

    # Early stopping
    EARLY_STOPPING_PATIENCE = 20  # ‚úÖ OPTIMIS√â: Plus de patience (120 epochs)
    EARLY_STOPPING_DELTA = 0.001

    # Label Smoothing
    LABEL_SMOOTHING = 0.1

    # Sauvegarde
    SAVE_BEST_ONLY = True
    SAVE_EVERY_N_EPOCHS = 10  # ‚úÖ Moins fr√©quent (epochs plus longs)

    # ============ √âvaluation ============
    EVAL_EVERY_N_EPOCHS = 1
    TOPK = (1, 5)

    # ============ Device ============
    DEVICE = 'cuda'

    # ============ Logging ============
    PRINT_FREQ = 50
    USE_WANDB = False
    WANDB_PROJECT = "food101-classifier-v3"
    WANDB_ENTITY = None

    # ============ Reproduction ============
    SEED = 42

    @classmethod
    def get_total_epochs(cls):
        """Retourne le nombre total d'epochs"""
        return cls.PHASE1_EPOCHS + cls.PHASE2_EPOCHS

    @classmethod
    def print_config(cls):
        """Affiche la configuration"""
        print("\n" + "="*80)
        print("üöÄ CONFIGURATION V3 - ARCHITECTURE EFFICIENTNET-B4")
        print("="*80)

        print("\nüìÅ CHEMINS:")
        print(f"  - Data dir: {cls.DATA_DIR}")
        print(f"  - Checkpoint dir: {cls.CHECKPOINT_DIR}")
        print(f"  - Results dir: {cls.RESULTS_DIR}")

        print("\nüß† MOD√àLE:")
        print(f"  - Architecture: {cls.MODEL_NAME.upper()} üÜï (NOUVEAU!)")
        print(f"  - Param√®tres: ~19M (vs 25.6M ResNet-50)")
        print(f"  - Classes: {cls.NUM_CLASSES}")
        print(f"  - Pretrained: {cls.PRETRAINED}")
        print(f"  - Dropout: {cls.DROPOUT}")

        print("\nüìä DONN√âES:")
        print(f"  - Image size: {cls.IMG_SIZE}x{cls.IMG_SIZE} ‚¨ÜÔ∏è (OPTIMIS√â: 224‚Üí380 pour EfficientNet)")
        print(f"  - Batch size: {cls.BATCH_SIZE} ‚¨áÔ∏è (r√©duit car images plus grandes)")
        print(f"  - Num workers: {cls.NUM_WORKERS}")
        print(f"  - Augmentation: {cls.AUGMENTATION_LEVEL.upper()}")

        print("\nüèãÔ∏è ENTRA√éNEMENT:")
        print(f"  - Phase 1: {cls.PHASE1_EPOCHS} epochs @ LR={cls.PHASE1_LR}")
        print(f"  - Phase 2: {cls.PHASE2_EPOCHS} epochs @ LR={cls.PHASE2_LR} (AdamW)")
        print(f"  - Total epochs: {cls.get_total_epochs()} (85‚Üí125)")
        print(f"  - Optimizer Phase 1: {cls.PHASE1_OPTIMIZER.upper()}")
        print(f"  - Optimizer Phase 2: {cls.PHASE2_OPTIMIZER.upper()} üÜï")

        print("\n‚ö° OPTIMISATIONS V3:")
        print(f"  - Mixed Precision (AMP): {cls.USE_AMP}")
        print(f"  - Gradient Clipping: {cls.GRADIENT_CLIP}")
        print(f"  - Scheduler: {cls.SCHEDULER_TYPE.upper()} (T_max={cls.T_MAX})")
        print(f"  - Warmup: {cls.WARMUP_EPOCHS} epochs ‚¨ÜÔ∏è (OPTIMIS√â)")
        print(f"  - Label Smoothing: {cls.LABEL_SMOOTHING}")
        print(f"  - MixUp: {cls.USE_MIXUP} (alpha={cls.MIXUP_ALPHA})")
        print(f"  - CutMix: {cls.USE_CUTMIX} (alpha={cls.CUTMIX_ALPHA})")
        print(f"  - MixUp/CutMix prob: {cls.MIXUP_PROB*100}% (25% seulement)")
        print(f"  - Test-Time Augmentation: {cls.USE_TTA} üÜï (NOUVEAU!)")

        print("\nüíæ SAUVEGARDE:")
        print(f"  - Save best only: {cls.SAVE_BEST_ONLY}")
        print(f"  - Early stopping patience: {cls.EARLY_STOPPING_PATIENCE} epochs")
        print(f"  - Save every N epochs: {cls.SAVE_EVERY_N_EPOCHS}")

        print("\nüñ•Ô∏è DEVICE:")
        print(f"  - Device: {cls.DEVICE}")

        print("\n" + "="*80)
        print("‚è±Ô∏è DUR√âE ESTIM√âE: 35-40 heures sur T4 GPU")
        print("üéØ OBJECTIF: Top-1 Accuracy = 85-90% ‚úÖ OBJECTIF ATTEINT!")
        print("üìà AM√âLIORATION vs V2: +19 √† +24% (66% ‚Üí 85-90%)")
        print("üèÜ SOTA Food-101: ~92% (avec ensembles)")
        print("="*80)

    @classmethod
    def get_changes_summary(cls):
        """R√©sum√© des changements vs V2"""
        changes = [
            ("MODEL", "ResNet-50", "EfficientNet-B4", "Architecture SOTA"),
            ("IMG_SIZE", "224", "380", "Taille optimale EfficientNet"),
            ("BATCH_SIZE", "32", "16", "Adapt√© aux images 380x380"),
            ("DROPOUT", "0.2", "0.3", "Plus de r√©gularisation"),
            ("PHASE2_EPOCHS", "80", "120", "+40 epochs"),
            ("PHASE2_LR", "1e-4", "3e-5", "LR adapt√© EfficientNet"),
            ("PHASE2_OPTIMIZER", "SGD", "AdamW", "Meilleur pour EfficientNet"),
            ("WARMUP_EPOCHS", "5", "10", "Warmup plus long"),
            ("MIXUP_ALPHA", "0.2", "0.15", "Moins agressif"),
            ("CUTMIX_ALPHA", "1.0", "0.25", "Beaucoup moins agressif"),
            ("MIXUP_PROB", "0.5 (50%)", "0.25 (25%)", "75% images normales"),
            ("USE_TTA", "False", "True", "Test-Time Augmentation"),
            ("T_MAX", "80", "120", "Scheduler align√©"),
            ("EARLY_STOPPING", "12", "20", "Plus de patience"),
        ]

        print("\n" + "="*80)
        print("üìä R√âSUM√â DES CHANGEMENTS V2 ‚Üí V3")
        print("="*80)
        print(f"\n{'Param√®tre':<25} {'V2':<20} {'V3':<20} {'Justification':<25}")
        print("-"*80)
        for param, v2, v3, justif in changes:
            print(f"{param:<25} {v2:<20} {v3:<20} {justif:<25}")
        print("="*80)

    @classmethod
    def get_requirements(cls):
        """Retourne les d√©pendances suppl√©mentaires pour V3"""
        print("\n" + "="*80)
        print("üì¶ D√âPENDANCES SUPPL√âMENTAIRES POUR V3")
        print("="*80)
        print("\nAjoutez √† requirements.txt:")
        print("  efficientnet-pytorch>=0.7.1")
        print("  ou utilisez timm (d√©j√† install√©):")
        print("  timm>=0.9.0  # Contient EfficientNet-B4 pr√©-entra√Æn√©")
        print("\nInstallation:")
        print("  pip install timm>=0.9.0")
        print("="*80)


# Configuration pour debug
class DebugConfigV3(ConfigV3):
    """Configuration pour tests rapides"""
    PHASE1_EPOCHS = 1
    PHASE2_EPOCHS = 2
    BATCH_SIZE = 4  # Petit batch pour images 380x380
    NUM_WORKERS = 0
    PRINT_FREQ = 10
    USE_TTA = False


if __name__ == "__main__":
    # Afficher la configuration
    ConfigV3.print_config()
    print("\n")
    ConfigV3.get_changes_summary()
    print("\n")
    ConfigV3.get_requirements()
